{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTENTIONAL FUSION CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   2%|‚ñè         | 83/5000 [00:02<02:38, 30.98it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                             precision_recall_fscore_support, accuracy_score, \n",
    "                             balanced_accuracy_score, matthews_corrcoef, \n",
    "                             cohen_kappa_score, f1_score)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path.cwd() if 'notebooks' not in str(Path.cwd()) else Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "AUDIO_DIR = DATA_DIR / 'processed' / 'birdcall_segments_5s_TOP50'\n",
    "FEATURES_DIR = DATA_DIR / 'features/birdcall_features_TOP50.csv'\n",
    "MODEL_DIR = BASE_DIR / 'models'  # New directory for model storage\n",
    "METRICS_DIR = BASE_DIR / 'metrics'  # New directory for metrics storage\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# Audio parameters\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 5\n",
    "N_MELS = 128\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "MAX_PAD_LEN = 259\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "INIT_LR = 1e-4\n",
    "\n",
    "def clean_filename(fname):\n",
    "    return os.path.splitext(os.path.basename(fname))[0]\n",
    "\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    \"\"\"Audio preprocessing without augmentation\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "        \n",
    "        # Mel spectrogram\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS,\n",
    "                                         n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "        \n",
    "        # Normalize\n",
    "        S_dB = (S_dB - np.min(S_dB)) / (np.max(S_dB) - np.min(S_dB) + 1e-8)\n",
    "        \n",
    "        # Padding\n",
    "        if S_dB.shape[1] < MAX_PAD_LEN:\n",
    "            pad_width = MAX_PAD_LEN - S_dB.shape[1]\n",
    "            S_dB = np.pad(S_dB, ((0,0), (0,pad_width)), mode='constant')\n",
    "        else:\n",
    "            S_dB = S_dB[:, :MAX_PAD_LEN]\n",
    "            \n",
    "        return S_dB[..., np.newaxis]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "class AudioFeatureGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Custom data generator without augmentation\"\"\"\n",
    "    def __init__(self, audio_data, feature_data, labels, batch_size=32, shuffle=True):\n",
    "        self.audio_data = audio_data\n",
    "        self.feature_data = feature_data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(audio_data))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.labels) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X_audio = []\n",
    "        X_features = []\n",
    "        y = []\n",
    "        \n",
    "        for i in batch_indices:\n",
    "            X_audio.append(self.audio_data[i])\n",
    "            X_features.append(self.feature_data[i])\n",
    "            y.append(self.labels[i])\n",
    "            \n",
    "        # Convert labels to one-hot encoding\n",
    "        y_onehot = to_categorical(y, num_classes=len(np.unique(self.labels)))\n",
    "        return [np.array(X_audio), np.array(X_features)], y_onehot\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "def build_enhanced_model(audio_shape, feature_dim, num_classes):\n",
    "    \"\"\"Model architecture without augmentation-related components\"\"\"\n",
    "    # Audio branch\n",
    "    audio_input = Input(shape=audio_shape, name='audio_input')\n",
    "    \n",
    "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(audio_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    audio_output = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    # Feature branch\n",
    "    feature_input = Input(shape=(feature_dim,), name='feature_input')\n",
    "    f = layers.Dense(128, activation='relu')(feature_input)\n",
    "    f = layers.BatchNormalization()(f)\n",
    "    feature_output = layers.Dense(64, activation='relu')(f)\n",
    "\n",
    "    # Cross-modal attention\n",
    "    attention = layers.Attention()([audio_output[:, None], feature_output[:, None]])\n",
    "    attention = layers.Flatten()(attention)\n",
    "\n",
    "    # Fusion\n",
    "    merged = layers.concatenate([audio_output, feature_output, attention])\n",
    "    \n",
    "    # Classifier\n",
    "    x = layers.Dense(512, activation='relu')(merged)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    output = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[audio_input, feature_input], outputs=output)\n",
    "\n",
    "    # Focal loss\n",
    "    def focal_loss(gamma=2., alpha=0.25):\n",
    "        def loss_fn(y_true, y_pred):\n",
    "            y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "            ce = -y_true * tf.math.log(y_pred)\n",
    "            weight = tf.pow(1 - y_pred, gamma) * y_true\n",
    "            fl = ce * weight * alpha\n",
    "            return tf.reduce_mean(tf.reduce_sum(fl, axis=-1))\n",
    "        return loss_fn\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=INIT_LR),\n",
    "        loss=focal_loss(),\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_dataset(features_df, top_features):\n",
    "    \"\"\"Create dataset without augmentation\"\"\"\n",
    "    wav_files = [f for f in os.listdir(AUDIO_DIR) if f.endswith('.wav')]\n",
    "    \n",
    "    X_audio = []\n",
    "    X_features = []\n",
    "    y = []\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(features_df['species'])\n",
    "    \n",
    "    feature_scaler = StandardScaler()\n",
    "    features = features_df[top_features].values\n",
    "    feature_scaler.fit(features)\n",
    "    \n",
    "    for wav_file in tqdm(wav_files, desc=\"Processing files\"):\n",
    "        file_id = clean_filename(wav_file)\n",
    "        try:\n",
    "            row = features_df[features_df['filename'].str.contains(file_id)].iloc[0]\n",
    "            audio_path = os.path.join(AUDIO_DIR, wav_file)\n",
    "            spec = load_and_preprocess_audio(audio_path)\n",
    "            if spec is None:\n",
    "                continue\n",
    "                \n",
    "            features = row[top_features].values.astype(np.float32)\n",
    "            features = feature_scaler.transform(features.reshape(1, -1))[0]\n",
    "            \n",
    "            X_audio.append(spec)\n",
    "            X_features.append(features)\n",
    "            y.append(label_encoder.transform([row['species']])[0])\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {file_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nLoaded {len(y)} samples\")\n",
    "    return np.array(X_audio), np.array(X_features), np.array(y), label_encoder\n",
    "\n",
    "def train_model(model, train_gen, val_gen, class_weights):\n",
    "    \"\"\"Training function\"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_auc', patience=20, mode='max', restore_best_weights=True),\n",
    "        ModelCheckpoint('AFCNN.h5', monitor='val_auc', save_best_only=True, mode='max'),\n",
    "        ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, min_lr=1e-6, mode='max')\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, test_gen, label_encoder):\n",
    "    \"\"\"Enhanced evaluation function with comprehensive metrics\"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred_proba = []\n",
    "    \n",
    "    for batch in test_gen:\n",
    "        X_batch, y_batch = batch\n",
    "        batch_pred = model.predict(X_batch)\n",
    "        y_true.extend(np.argmax(y_batch, axis=1))\n",
    "        y_pred.extend(np.argmax(batch_pred, axis=1))\n",
    "        y_pred_proba.extend(batch_pred)\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    \n",
    "    # Convert to one-hot for multiclass metrics\n",
    "    y_onehot = to_categorical(y_true, num_classes=len(label_encoder.classes_))\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'matthews_corrcoef': matthews_corrcoef(y_true, y_pred),\n",
    "        'cohen_kappa': cohen_kappa_score(y_true, y_pred),\n",
    "        'auc_ovr': roc_auc_score(y_onehot, y_pred_proba, multi_class='ovr'),\n",
    "        'auc_ovo': roc_auc_score(y_onehot, y_pred_proba, multi_class='ovo'),\n",
    "    }\n",
    "    \n",
    "    # Add per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n",
    "    metrics['per_class'] = {\n",
    "        'precision': precision.tolist(),\n",
    "        'recall': recall.tolist(),\n",
    "        'f1': f1.tolist(),\n",
    "        'support': support.tolist()\n",
    "    }\n",
    "    \n",
    "    # Weighted and macro averages\n",
    "    metrics['weighted_avg'] = {\n",
    "        'precision': precision_recall_fscore_support(y_true, y_pred, average='weighted')[0],\n",
    "        'recall': precision_recall_fscore_support(y_true, y_pred, average='weighted')[1],\n",
    "        'f1': precision_recall_fscore_support(y_true, y_pred, average='weighted')[2]\n",
    "    }\n",
    "    \n",
    "    metrics['macro_avg'] = {\n",
    "        'precision': precision_recall_fscore_support(y_true, y_pred, average='macro')[0],\n",
    "        'recall': precision_recall_fscore_support(y_true, y_pred, average='macro')[1],\n",
    "        'f1': precision_recall_fscore_support(y_true, y_pred, average='macro')[2]\n",
    "    }\n",
    "    \n",
    "    # Save metrics to JSON file\n",
    "    metrics_file = METRICS_DIR / 'evaluation_metrics_AFCNN.json'\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    # Print summary metrics\n",
    "    print(\"\\nEvaluation Metrics Summary:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"Matthews Correlation: {metrics['matthews_corrcoef']:.4f}\")\n",
    "    print(f\"Cohen's Kappa: {metrics['cohen_kappa']:.4f}\")\n",
    "    print(f\"AUC (OvR): {metrics['auc_ovr']:.4f}\")\n",
    "    print(f\"AUC (OvO): {metrics['auc_ovo']:.4f}\")\n",
    "    print(f\"Weighted Avg F1: {metrics['weighted_avg']['f1']:.4f}\")\n",
    "    print(f\"Macro Avg F1: {metrics['macro_avg']['f1']:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_encoder.classes_,\n",
    "               yticklabels=label_encoder.classes_, cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(METRICS_DIR / 'confusion_matrix_AFCNN.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and prepare data\n",
    "    features_df = pd.read_csv(FEATURES_DIR)\n",
    "    \n",
    "     # Predefined top features\n",
    "    top_features = [\n",
    "    'lowlevel.spectral_energyband_middle_low.stdev',\n",
    "    'lowlevel.hfc.stdev',\n",
    "    'lowlevel.pitch_salience.stdev',\n",
    "    'lowlevel.spectral_centroid.mean',\n",
    "    'lowlevel.spectral_energyband_high.stdev',\n",
    "    'lowlevel.spectral_complexity.mean',\n",
    "    'lowlevel.spectral_decrease.stdev',\n",
    "    'lowlevel.spectral_strongpeak.stdev',\n",
    "    'lowlevel.spectral_complexity.stdev',\n",
    "    'lowlevel.spectral_energyband_middle_low.mean',\n",
    "    'lowlevel.spectral_strongpeak.mean',\n",
    "    'lowlevel.loudness_ebu128.integrated',\n",
    "    'lowlevel.erbbands_skewness.mean',\n",
    "    'lowlevel.pitch.mean',\n",
    "    'lowlevel.spectral_flux.stdev',\n",
    "    'lowlevel.spectral_rolloff.mean',\n",
    "    'lowlevel.spectral_energyband_middle_high.mean',\n",
    "    'lowlevel.pitch_salience.mean'\n",
    "]\n",
    "    \n",
    "    # Create dataset\n",
    "    X_audio, X_features, y, label_encoder = create_dataset(features_df, top_features)\n",
    "    \n",
    "    # Split data\n",
    "    (X_audio_train, X_audio_test, \n",
    "     X_feat_train, X_feat_test,\n",
    "     y_train, y_test) = train_test_split(\n",
    "        X_audio, X_features, y,\n",
    "        test_size=0.3,\n",
    "        stratify=y,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create data generators\n",
    "    train_gen = AudioFeatureGenerator(X_audio_train, X_feat_train, y_train, BATCH_SIZE)\n",
    "    val_gen = AudioFeatureGenerator(X_audio_test, X_feat_test, y_test, BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Build model\n",
    "    model = build_enhanced_model(\n",
    "        audio_shape=X_audio.shape[1:],\n",
    "        feature_dim=len(top_features),\n",
    "        num_classes=len(label_encoder.classes_)\n",
    "    )\n",
    "    \n",
    "    # Update ModelCheckpoint to save in MODEL_DIR\n",
    "    model_file = MODEL_DIR / 'TRAIN_AFCNN.h5'\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_auc', patience=20, mode='max', restore_best_weights=True),\n",
    "        ModelCheckpoint(model_file, monitor='val_auc', save_best_only=True, mode='max'),\n",
    "        ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, min_lr=1e-6, mode='max')\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_file = MODEL_DIR / 'FINAL_AFCNN.h5'\n",
    "    model.save(final_model_file)\n",
    "    print(f\"\\nFinal model saved to {final_model_file}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_file = METRICS_DIR / 'training_history_AFCNN.json'\n",
    "    with open(history_file, 'w') as f:\n",
    "        json.dump(history.history, f, indent=4)\n",
    "    \n",
    "    # Evaluate final model\n",
    "    print(\"\\nEvaluating Best Model:\")\n",
    "    metrics = evaluate_model(model, val_gen, label_encoder)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['auc'], label='Train AUC')\n",
    "    plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "    plt.title('Training AUC')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(METRICS_DIR / 'training_history-AFCNN.png', dpi=300)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
